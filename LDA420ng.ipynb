{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA420ng.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPqGz7kPZ7E7mcyMheQ1Kp3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SONGZERUIgit/KeyWords/blob/main/LDA420ng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lnk0cVwddQ9"
      },
      "source": [
        "import pandas as pd\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "l=len(documents)\n",
        "news_df = pd.DataFrame({'document':documents})\n",
        "# removing everything except alphabets`\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "# removing short words\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "# make all text lowercase\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "# tokenization\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
        "# remove stop-words\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "# de-tokenization\n",
        "dictionary = corpora.Dictionary(tokenized_doc)\n",
        "\n",
        "# convert tokenized documents into a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
        "print(corpus)\n",
        "# generate LDA model\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=50, id2word=dictionary, passes=30)\n",
        "print(ldamodel.print_topics(num_topics=50, num_words=8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z_uPxcZZ_KN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}